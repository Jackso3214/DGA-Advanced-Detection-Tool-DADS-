{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ba9fd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacks\\AppData\\Local\\Temp\\ipykernel_2676\\1905729114.py:160: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  hom_data = home_data.append(X3, ignore_index=True)\n",
      "C:\\Users\\Jacks\\AppData\\Local\\Temp\\ipykernel_2676\\1905729114.py:161: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ver_data=verd_data.append(X2, ignore_index=True)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'KMeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 211\u001b[0m\n\u001b[0;32m    207\u001b[0m verd_data\u001b[38;5;241m=\u001b[39mverd_data\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVN\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    210\u001b[0m home_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/Jacks/Downloads/Capstone Unsupervised/outputTable10k_set1.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,usecols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m column: column \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m columns_to_exclude)   \n\u001b[1;32m--> 211\u001b[0m \u001b[43mkmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mww.com\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 173\u001b[0m, in \u001b[0;36mkmean\u001b[1;34m(indomain)\u001b[0m\n\u001b[0;32m    171\u001b[0m X_principal \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_principal)\n\u001b[0;32m    172\u001b[0m X_principal\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 173\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m \u001b[43mKMeans\u001b[49m(n_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, n_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    174\u001b[0m kmeans\u001b[38;5;241m.\u001b[39mfit(X_principal)\n\u001b[0;32m    175\u001b[0m ver_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mkmeans\u001b[38;5;241m.\u001b[39mlabels_\n",
      "\u001b[1;31mNameError\u001b[0m: name 'KMeans' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import RFE\n",
    "import re\n",
    "import os.path\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "#custom imports\n",
    "import EntropyCalculator as ent\n",
    "import VowelConsonantsCalculator as vowcon\n",
    "import FeatureEngineering as feat\n",
    "import DomainNgrams as dngrams\n",
    "import NumCountCalculator as numCount\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "\n",
    "def load_DGA_data():\n",
    "    return pd.read_csv(\"CSVs\\\\trainingDataset\\\\dga_dataset_10k_set1.csv\")\n",
    "\n",
    "def Export_DGA_data(dataframe):\n",
    "    dataframe.to_csv(\"CSVs\\output\\outputTable10k_set1.csv\")\n",
    "\n",
    "def readNgram(path):\n",
    "    temp = []\n",
    "    with open(path) as file:\n",
    "                    reader = csv.reader(file)\n",
    "                    for row in reader:\n",
    "                        temp.append(row)\n",
    "                    file.close()\n",
    "    return temp\n",
    "\n",
    "def prepDataTest(string):\n",
    "    #assign string\n",
    "    domain = string\n",
    "\n",
    "    #create temporary lists\n",
    "    top2String = []\n",
    "    top3String = []\n",
    "    top4String = []\n",
    "    top5String = []\n",
    "\n",
    "    #calculate and create an Entropy column\n",
    "    entropy = ent.Entropy(domain)\n",
    "    \n",
    "\n",
    "    #calculate and create an vowerl and consonants column\n",
    "    count = vowcon.VowelConsonants(domain)\n",
    "    vowel = count['vowel']\n",
    "    consonant = count['consonant']\n",
    "    #calculate and create an Length column\n",
    "    \n",
    "    length = len(domain)\n",
    "\n",
    "    #convert verdict to a number column\n",
    "    vowelConsRatio = round(vowel / consonant, 5)\n",
    "    entropyLenRatio = round(entropy / length, 5)\n",
    "\n",
    "    #calculate numcount and create a column\n",
    "    countNum = numCount.numCount(domain)\n",
    "\n",
    "    #calculate and create a ngram score column FOR LEGIT NGRAMS\n",
    "    #if topNgrams exist in folder, read those otherwise generate it\n",
    "    #generate top NGrams for legit\n",
    "    #if file exists in the specific folder\n",
    "    #top 2 ngrams\n",
    "    #if folder exists\n",
    "   \n",
    "        #check to see if the csv file is in the same folder as python file\n",
    "            #read the file if it is\n",
    "    top2String = readNgram(\"top2String.csv\")\n",
    "    top3String = readNgram(\"top3String.csv\")\n",
    "    top4String = readNgram(\"top4String.csv\")\n",
    "    top5String = readNgram(\"top5String.csv\")\n",
    "\n",
    "    testDomain = re.split('.[A-Za-z]*\\.*$', domain)[0]\n",
    "    \n",
    "    totalScore = 0\n",
    "    score = [0,0,0,0]\n",
    "    for ngram in top2String:\n",
    "        if ngram[0] in testDomain:\n",
    "            totalScore += 1\n",
    "            score[0] += 1\n",
    "\n",
    "    for ngram in top3String:\n",
    "        if ngram[0] in testDomain:\n",
    "            totalScore += 1\n",
    "            score[1] += 1\n",
    "\n",
    "    for ngram in top4String:\n",
    "        if ngram[0] in testDomain:\n",
    "            totalScore += 1\n",
    "            score[2] += 1\n",
    "\n",
    "    for ngram in top5String:\n",
    "        if ngram[0] in testDomain:\n",
    "            totalScore += 1\n",
    "            score[3] += 1\n",
    "\n",
    "    #calculate and create a ngram score column FOR DGA NGRAMS\n",
    "    \n",
    "    #if topNgrams exist in folder, read those otherwise generate it\n",
    "    #generate top NGrams for legit\n",
    "    #if file exists in the specific folder\n",
    "    #top 2 ngrams\n",
    "    #if folder exists\n",
    "  \n",
    "        #check to see if the csv file is in the same folder as python file\n",
    "            #read the file if it is\n",
    "    top2String = readNgram(\"top2StringDGA.csv\")\n",
    "    top3String = readNgram(\"top3StringDGA.csv\")\n",
    "    top4String = readNgram(\"top4StringDGA.csv\")\n",
    "    top5String = readNgram(\"top5StringDGA.csv\")\n",
    "\n",
    "    testDomain = re.split('.[A-Za-z]*\\.*$', domain)[0]\n",
    "    \n",
    "    totalScoreDGA = 0\n",
    "    scoreDGA = [0,0,0,0]\n",
    "    for ngram in top2String:\n",
    "        if ngram[0] in testDomain:\n",
    "            totalScoreDGA += 1\n",
    "            scoreDGA[0] += 1\n",
    "\n",
    "    for ngram in top3String:\n",
    "        if ngram[0] in testDomain:\n",
    "            totalScoreDGA += 1\n",
    "            scoreDGA[1] += 1\n",
    "\n",
    "    for ngram in top4String:\n",
    "        if ngram[0] in testDomain:\n",
    "            totalScoreDGA += 1\n",
    "            scoreDGA[2] += 1\n",
    "\n",
    "    for ngram in top5String:\n",
    "        if ngram[0] in testDomain:\n",
    "            totalScoreDGA += 1\n",
    "            scoreDGA[3] += 1\n",
    "    \n",
    "    #note: Domain,Verdict,Entropy,Vowel,Consonant,Length,VN,VowelConsRatio,EntropyLength,ngram2Score,ngram3Score,ngram4Score,ngram5Score,ngramTotalScore\n",
    "    data = [[string,\"unknown\",entropy, vowel, consonant, length, vowelConsRatio, entropyLenRatio, countNum ,score[0], score[1], score[2], score[3], totalScore, scoreDGA[0], scoreDGA[1], scoreDGA[2], scoreDGA[3], totalScoreDGA]]\n",
    "    df = pd.DataFrame(data, columns=[\"Domain\",\"Verdict\",\"Entropy\", \"Vowel\", \"Consonant\", \"Length\", \"VowelConsRatio\", \"EntropyLength\", \"CountNum\" ,\"ngram2ScoreLegit\", \"ngram3ScoreLegit\", \"ngram4ScoreLegit\", \"ngram5ScoreLegit\", \"ngramTotalScoreLegit\", \"ngram2ScoreDGA\", \"ngram3ScoreDGA\", \"ngram4ScoreDGA\", \"ngram5ScoreDGA\", \"ngramTotalScoreDGA\"])\n",
    "    return df\n",
    "\n",
    "def kmean(indomain):\n",
    "    X=prepDataTest(indomain)\n",
    "    X2 = X[['Domain', 'Verdict']]\n",
    "    X3 = X.iloc[:, 2:]\n",
    "    hom_data = home_data.append(X3, ignore_index=True)\n",
    "    ver_data=verd_data.append(X2, ignore_index=True)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(home_data)\n",
    " \n",
    "    X_normalized = normalize(X_scaled)\n",
    " \n",
    "    X_normalized = pd.DataFrame(X_normalized)\n",
    " \n",
    "    pca = PCA(n_components = 2)\n",
    "    X_principal = pca.fit_transform(X_normalized)\n",
    "    X_principal = pd.DataFrame(X_principal)\n",
    "    X_principal.columns = ['P1', 'P2']\n",
    "    kmeans = KMeans(n_clusters = 4, random_state = 0, n_init='auto')\n",
    "    kmeans.fit(X_principal)\n",
    "    ver_data['labels']=kmeans.labels_\n",
    "    value_counts = ver_data['labels'].value_counts()##\n",
    "    grouped = ver_data.groupby(['Verdict', 'labels'])\n",
    "    result = grouped.size()##\n",
    "    return result\n",
    "\n",
    "def spect(indomain, affin='rbf'):\n",
    "    X=prepDataTest(indomain)\n",
    "    X2 = X[['Domain', 'Verdict']]\n",
    "    X3 = X.iloc[:, 2:]\n",
    "    hom_data = home_data.append(X3, ignore_index=True)\n",
    "    ver_data=verd_data.append(X2, ignore_index=True)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(hom_data) \n",
    "    X_normalized = normalize(X_scaled)\n",
    "    X_normalized = pd.DataFrame(X_normalized)\n",
    "    pca = PCA(n_components = 2)\n",
    "    X_principal = pca.fit_transform(X_normalized)\n",
    "    X_principal = pd.DataFrame(X_principal)\n",
    "    X_principal.columns = ['P1', 'P2']\n",
    "    spectral_model = SpectralClustering(n_clusters = 4, affinity =affin)\n",
    "    labels = spectral_model.fit_predict(X_principal)\n",
    "    ver_data=ver_data\n",
    "    ver_data['labels']=labels\n",
    "    value_counts = ver_data['labels'].value_counts()##\n",
    "    grouped = ver_data.groupby(['Verdict', 'labels'])\n",
    "    result = grouped.size()\n",
    "    return result\n",
    "    \n",
    "columns_to_exclude = ['Domain', 'Verdict', 'VN','Unnamed: 0']\n",
    "verd_data = pd.read_csv('C:/Users/Jacks/Downloads/Capstone Unsupervised/outputTable10k_set1.csv',usecols=lambda column: column in columns_to_exclude)\n",
    "verd_data=verd_data.drop(columns='Unnamed: 0')\n",
    "verd_data=verd_data.drop(columns='VN')\n",
    "\n",
    "\n",
    "home_data = pd.read_csv('C:/Users/Jacks/Downloads/Capstone Unsupervised/outputTable10k_set1.csv',usecols=lambda column: column not in columns_to_exclude)   \n",
    "kmean('ww.com')\n",
    "##spect('ww.com', 'rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31589fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
